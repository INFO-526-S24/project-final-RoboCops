---
title: "Analysis and Visualization of Police Shootings in the United States"
subtitle: "INFO 526 - Fall 2023 - Project Final"
author: "Peter Yeh, Sachin Pandurang Patil, Arun Koundinya Parasa, Surajit Pal, Christian Ortmann, Val√©rie Naa Dei Okine"
title-slide-attributes:
  data-background-image: images/skyline_pic.jpg
  data-background-size: stretch
  data-background-opacity: "0.5"
  data-slide-number: none
format:
  revealjs:
    theme:  ['data/customtheming.scss']
    tbl-colwidths: auto
  
editor: visual
execute:
  echo: false
---

```{r}
#| label: load-packages
#| include: false

# Load packages here
pacman::p_load(dplyr,
               ggplot2,
               ggridges,
               scales,
               patchwork,
               stringr,
               grep,
               kableExtra,
               plotly, 
               tidyverse)

```

```{r}
#| label: setup
#| include: false

# Plot theme
ggplot2::theme_set(ggplot2::theme_minimal(base_size = 11))

# For better figure resolution
knitr::opts_chunk$set(
  fig.retina = 3, 
  dpi = 300, 
  fig.width = 6, 
  fig.asp = 0.618 
  )
```

```{r}
#| label: load-dataset
#| message: false

# Load in the datasets
fatality <- read.csv("data/PoliceKillingsUS.csv", na.strings = "")
median_income <- read.csv("data/MedianHouseholdIncome2015.csv")
poverty_perc <- read.csv("data/PercentagePeopleBelowPovertyLevel.csv")
hs_perc <- read.csv("data/PercentOver25CompletedHighSchool.csv")
race_perc <- read.csv("data/ShareRaceByCity.csv")
city_pop <- read.csv("data/us-cities-top-1k-multi-year.csv")
state_pop <- read.csv("data/Population_Estimate_data_Statewise_2010-2023.csv")
```

# Project Overview and Dataset

## Police Shootings: An overview
- Police shootings are an unfortunate aspect of enforcing the law, as any loss of life is a tragic incident
- We aim to visualize and analyze police shootings in the United States from 2015 to 2017
- The data points were gathered on Kaggle and sourced from the Washington Post, so we are limited to their interpretation of what counts a as a police shooting

## Dataset Variables {style="font-size: 55%"}
```{r}

#| label: dataset-table
#| echo: false
#| message: false
#| 


# Create columns for data
fatality_var <- colnames(fatality)

fatality_desc <- c("Data ID",
                   "Name of deceased",
                   "Date of shooting",
                   "Means of death",
                   "Weapon/Tool",
                   "Age of deceased",
                   "Gender of deceased",
                   "Race of deceased",
                   "City",
                   "State",
                   "Sign of mental illness during incident",
                   "Deceased's threat level",
                   "Method to evade police",
                   "Reports of police with body camera")

fatality_type <- c("numeric",
                   "character",
                   "date",
                   "categorical",
                   "categorical",
                   "numeric",
                   "logical",
                   "categorical",
                   "categorical",
                   "categorical",
                   "logical",
                   "categorical",
                   "categorical",
                   "logical")

fatality_values <- c("2535 entries",
                     "Names",
                     "dd/mm/yy from 2015-2017",
                     "shot, shot and tasered",
                     "69 categories",
                     "6-91 years old",
                     "F/M",
                     "A, B, H, N, O, W, Unknown",
                     "1417 cities",
                     "State 2 letter abbreviations and DC",
                     "True/False",
                     "attack, other, undetermined",
                     "Car, Foot, Not fleeing, Other",
                     "True/False")

# Bind the vectors into a dataframe
fatality_table <- as.data.frame(cbind(fatality_var,
                                      fatality_desc,
                                      fatality_type,
                                     fatality_values))
colnames(fatality_table) <- c("Variable", "Description", "Data Type", "Values")
row.names(fatality_table) <- seq(1:14)

# Generate a nice table with kable
my_table <- kable(fatality_table, "html") |>
  kable_styling(full_width = FALSE)

# Display the table
my_table

```

<!--------------------------------------------------------------->
# Data Summary with Dashboard


# Question 1

How do socioeconomic factors such as median income, poverty rates, and educational relate with incidents of police use of force across different geographic areas?

## Plot 1a

## Plot 1b

## Plot 2a
```{r}
#| label: Q1-plot2a
#| message: false

# Calculate the populations of each population for each city
supp_stat <- na.omit(supp_data)
supp_stat <- supp_stat |>
  mutate(
    white_pop <- White * Population / 100,
    black_pop <- Black * Population / 100,
    native_pop <- Native * Population / 100,
    asian_pop <- Asian * Population / 100,
    hispanic_pop <- Hispanic * Population / 100
  )

# Select specific columns of the supplementary data
supp_stat <- supp_stat[,c(1, 2, 4, 15:19)]
colnames(supp_stat) <- c("State",
                         "City",
                         "Median",
                         "White",
                         "Black",
                         "Native",
                         "Asian",
                         "Hispanic")

# Select specific columns of the fatality data
fatality_subset <- fatality[,8:10]
colnames(fatality_subset) <- c("Race", "City", "State")

# Merge dataset and remove rows with NAs
merged_data <- left_join(fatality_subset, supp_stat,
                         by = c("State", "City"))
merged_data <- na.omit(merged_data)

# Acquire counts of race:
# Asian = 23, Black = 409, Hispanic = 301, Native American = 13, Other = 17,
# Unknown = 92, White = 466
race_count <- as.data.frame(summary(as.factor(merged_data$Race)))

# Refactor levels according to the total counts
merged_data$Race <- factor(merged_data$Race,
                           levels = c("Unknown",
                                      "Other",
                                      "Native American",
                                      "Asian",
                                      "Hispanic",
                                      "Black",
                                      "White"))

# Generate the density ridge plot (smallest count at the bottom)
ggplot(merged_data, aes(x = Median, y = Race)) +
  geom_density_ridges() +
  scale_x_continuous(expand = c(0, 0),
                    labels = label_dollar(scale = 1e-3, suffix = "K")) +
  coord_cartesian(clip = "off") +
  geom_text(aes(x = 125000, y = 1.2, label = "13")) +
  geom_text(aes(x = 125000, y = 2.2, label = "17")) +
  geom_text(aes(x = 125000, y = 3.2, label = "23")) +
  geom_text(aes(x = 125000, y = 4.2, label = "92")) +
  geom_text(aes(x = 125000, y = 5.2, label = "301")) +
  geom_text(aes(x = 125000, y = 6.2, label = "409")) +
  geom_text(aes(x = 125000, y = 7.2, label = "466")) +
  labs(title = "Density Plots of Police Shootings",
       subtitle = "Ordered by descending counts") +
  xlab("Median Income of City") +
  ylab("Race") +
  theme_minimal()
```

## Plot 2b
```{r}
#| label: Q1-median
#| message: false

# Find distinct city and states and only keep numerical data
distinct_city <- merged_data |>
  distinct(State, City, .keep_all = TRUE) |>
  select(Median, White, Black, Native, Asian, Hispanic)

# Generate dataset for geom_smooth
distinct_city <- distinct_city |> # Find the proportions for each race
  mutate(
    Total = rowSums(across(White:Hispanic)),
    White = White / Total,
    Black = Black / Total,
    Native = Native / Total,
    Asian = Asian / Total,
    Hispanic = Hispanic / Total
  ) |> # Pivot data so race and values are columns
  pivot_longer(
    cols = c(White, Black, Native, Asian, Hispanic),
    names_to = "Race",
    values_to = "Percent"
  ) |> # Create legend column so that races can have colors later
  mutate(
    legend = case_when(
      Race %in% "White" ~ "White",
      Race %in% "Black" ~ "Black",
      TRUE ~ "Other"
    )
  )

# Refactor levels for legends order
distinct_city$legend <- factor(distinct_city$legend,
                               levels = c("Black", "White", "Other"))

# Generate smooth plot
ggplot(data = distinct_city,
       aes(x = Median, y = Percent, group = Race, color = legend)) +
  geom_smooth(se = F) +
  scale_color_manual(values = c("red", "blue", "grey")) +
  scale_x_continuous(labels = label_dollar(scale = 1e-3, suffix = "K")) +
  scale_y_continuous(labels = percent_format()) +
  coord_cartesian(xlim = c(35000, 75000)) +
  labs(title = "City Race Proportions",
       subtitle = "Over city median income",
       x = "Median Income of City",
       y = "Proportion of Population") +
  guides(color = guide_legend(title = "Race")) +
  theme_minimal() +
  theme()

```

## Plot 2c
```{r}
#| label: Q1-plot2b
#| message: false

# Find the race populations
# White = 135231935, Black = 30170898, Native = 1827873, Asian = 12570851,
# Hispanic = 51034800
race_pop <- merged_data |>
  distinct(State, City, .keep_all = TRUE)
race_total <- as.data.frame(colSums(race_pop[,5:9]))

# Proportion shot for each race:
# Asian:0.000182963
# race_count[1,1]/race_total[4,1] * 100
# Black: 0.001355611
# race_count[2,1]/race_total[2,1] * 100
# Hispanic: 0.0005897936
# race_count[3,1]/race_total[5,1] * 100
# Native: 0.0007112093
# race_count[4,1]/race_total[3,1] * 100
# White: 0.0003445932
# race_count[7,1]/race_total[1,1] * 100

# Refactor levels according to the proportions
merged_data$Race <- factor(merged_data$Race,
                           levels = c("Unknown",
                                      "Other",
                                      "Asian",
                                      "White",
                                      "Hispanic",
                                      "Native American",
                                      "Black"))

# Generate the density ridge plot (smallest proportion at the bottom)
ggplot(merged_data, aes(x = Median, y = Race)) +
  geom_density_ridges() +
  scale_x_continuous(expand = c(0, 0),
                    labels = label_dollar(scale = 1e-3, suffix = "K")) +
  coord_cartesian(clip = "off") +
  geom_text(aes(x = 125000, y = 3.2, label = "0.0002%")) +
  geom_text(aes(x = 125000, y = 4.2, label = "0.0003%")) +
  geom_text(aes(x = 125000, y = 5.2, label = "0.0006%")) +
  geom_text(aes(x = 125000, y = 6.2, label = "0.0007%")) +
  geom_text(aes(x = 125000, y = 7.2, label = "0.0014%")) +
  labs(title = "Density Plots of Police Shootings",
       subtitle = "Ordered by descending proportion within race") +
  xlab("Median Income of City") +
  ylab("Race") +
  theme_minimal()
```

## Insights and Knowledge Inferred {style="font-size: 80%"}

- Cities with greater than 75% high school graduates and poverty levels between 18-25% experience the most shootings with roughly half of all incidents showing a threat level of "Attack"

- Most shootings occur in cities where the median income is roughly between \$40k - \$50k per year, roughly the US average

- White people are shot the most, with the highest raw count, but proportional to the overall population of each race, black people are shot more

- 1 in every 7,143 black people are shot by the police compared to 1 in every 333,334 white people. Thus, black people are 47 times more likely to be shot by police, a striking and sad statistic
<!--------------------------------------------------------------->

# Question 2

## Plot 1

## Plot 2

## Insights and Knowledge Inferred

<!--------------------------------------------------------------->






















## Quarto

-   The presentation is created using the Quarto CLI

-   `##` sets the start of a new slide

## Layouts

You can use plain text

::: columns
::: {.column width="40%"}
-   or bullet points[^1]
:::

::: {.column width="60%"}
or in two columns
:::
:::

[^1]: And add footnotes

-   like

-   this

## Code

```{r, echo=FALSE}
# model <- lm(mpg ~ speed, data = mtcars) 
# 
# model |> tidy()
# 
# model |> glance()

```

## Plots

```{r}
# penguins |>
#   mutate(species = ifelse(species == "Adelie", "Adelie", "Other")) |>
#   ggplot(aes(x = flipper_length_mm, y = body_mass_g, color = species)) +
#   geom_point()
```

## Plot and text

::: columns
::: {.column width="50%"}
-   Some text

-   goes here
:::

::: {.column width="50%"}
```{r, warning=FALSE, fig.width=5.5}
# penguins |>
#   ggplot(aes(x = bill_length_mm, y = species, color = species)) +
#   geom_boxplot(linewidth = 0.75,
#                outlier.size = 2.5) +
#   theme_minimal(base_size = 15) +
#   theme(legend.key.size = unit(0.8, "cm"))
```
:::
:::

# A new section...

## Tables

If you want to generate a table, make sure it is in the HTML format (instead of Markdown or other formats), e.g.,

```{r}
# penguins |> 
#   head() |>
#   kableExtra::kable() |>
#   kableExtra::kable_styling()
```

## Images

![Image credit: Danielle Navarro, Percolate.](images/watercolour_sys02_img34_teacup-ocean.png){fig-align="center" width="500"}

## Math Expressions {.smaller}

You can write LaTeX math expressions inside a pair of dollar signs, e.g.¬†\$\\alpha+\\beta\$ renders $\alpha + \beta$. You can use the display style with double dollar signs:

```         
$$\bar{X}=\frac{1}{n}\sum_{i=1}^nX_i$$
```

$$
\bar{X}=\frac{1}{n}\sum_{i=1}^nX_i
$$

Limitations:

1.  The source code of a LaTeX math expression must be in one line, unless it is inside a pair of double dollar signs, in which case the starting `$$` must appear in the very beginning of a line, followed immediately by a non-space character, and the ending `$$` must be at the end of a line, led by a non-space character;

2.  There should not be spaces after the opening `$` or before the closing `$`.

# Wrap up

## Feeling adventurous?

-   You are welcomed to use the default styling of the slides. In fact, that's what I expect majority of you will do. You will differentiate yourself with the content of your presentation.

-   But some of you might want to play around with slide styling. Some solutions for this can be found at https://quarto.org/docs/presentations/revealjs.
