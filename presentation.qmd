---
title: "Analysis and Visualization of Police Shootings in the United States"
subtitle: "INFO 526 - Spring 2024 - Project Final"
author: "Peter Yeh, Sachin Pandurang Patil, Arun Koundinya Parasa, Surajit Pal, Christian Ortmann, Val√©rie Naa Dei Okine"
title-slide-attributes:
  data-background-image: images/skyline_pic.jpg
  data-background-size: stretch
  data-background-opacity: "0.5"
  data-slide-number: none
format:
  revealjs:
    theme:  [simple,data/customtheming.scss]
    transition: slide
    background-transition: fade
    logo: images/fatalshootings.jpeg
    footer: "[RoboCops](https://info-526-s24.github.io/project-final-RoboCops/)"
    scrollable: true
    tbl-colwidths: auto
  
editor: visual
execute:
  warning: false
  echo: false
---

```{r}
#| label: load-packages
#| include: false

# Load packages here
pacman::p_load(dplyr,
               ggplot2,
               ggridges,
               scales,
               patchwork,
               stringr,
               kableExtra,
               plotly, 
               tidyverse)

```

```{r}
#| label: setup
#| include: false

# Plot theme
ggplot2::theme_set(ggplot2::theme_minimal(base_size = 11))

# For better figure resolution
knitr::opts_chunk$set(
  fig.retina = 3, 
  dpi = 300, 
  fig.width = 6, 
  fig.asp = 0.618 
  )
```

```{r}
#| label: load-dataset
#| message: false

# Load in the datasets
fatality <- read.csv("data/PoliceKillingsUS.csv", na.strings = "")
median_income <- read.csv("data/MedianHouseholdIncome2015.csv")
poverty_perc <- read.csv("data/PercentagePeopleBelowPovertyLevel.csv")
hs_perc <- read.csv("data/PercentOver25CompletedHighSchool.csv")
race_perc <- read.csv("data/ShareRaceByCity.csv")
city_pop <- read.csv("data/us-cities-top-1k-multi-year.csv")
state_pop <- read.csv("data/Population_Estimate_data_Statewise_2010-2023.csv")
```




```{r}
#| label: data-setup
#| message: false
#| warning: FALSE
#| echo: FALSE

# Recode Fatality NA
fatality <- fatality |>
  mutate(
    race = case_when(
      race %in% "A" ~ "Asian",
      race %in% "B" ~ "Black",
      race %in% "H" ~ "Hispanic",
      race %in% "N" ~ "Native American",
      race %in% "O" ~ "Other",
      race %in% "W" ~ "White",
      is.na(race) == TRUE ~ "Unknown",
      TRUE ~ race
    )
  )

# Recode non-numeric values as "", remove NA
median_income <- median_income |>
  mutate(
    Median.Income = case_when(
      Median.Income %in% "-" ~ "",
      Median.Income %in% "(X)" ~ "",
      TRUE ~ Median.Income
    )
  )
median_income$Median.Income <- as.numeric(median_income$Median.Income)
median_income <- na.omit(median_income)

# Clean race data
race_perc[,3] <- as.numeric(race_perc[,3])
race_perc[,4] <- as.numeric(race_perc[,4])
race_perc[,5] <- as.numeric(race_perc[,5])
race_perc[,6] <- as.numeric(race_perc[,6])
race_perc[,7] <- as.numeric(race_perc[,7])
colnames(race_perc) <- c("Geographic.Area", "City",
                         "White", "Black", "Native", "Asian", "Hispanic")

# Adjust the state abbreviations to full name
state_pop <- state_pop |>
  mutate(
    STATE = case_when(
      STATE %in% "Alabama" ~ "AL",
      STATE %in% "Alaska" ~ "AK",
      STATE %in% "Arizona" ~ "AZ",
      STATE %in% "Arkansas" ~ "AR",
      STATE %in% "California" ~ "CA",
      STATE %in% "Colorado" ~ "CO",
      STATE %in% "Connecticut" ~ "CT",
      STATE %in% "Delaware" ~ "DE",
      STATE %in% "District of Columbia" ~ "DC",
      STATE %in% "Florida" ~ "FL",
      STATE %in% "Georgia" ~ "GA",
      STATE %in% "Hawaii" ~ "HI",
      STATE %in% "Idaho" ~ "ID",
      STATE %in% "Illinois" ~ "IL",
      STATE %in% "Indiana" ~ "IN",
      STATE %in% "Iowa" ~ "IA",
      STATE %in% "Kansas" ~ "KS",
      STATE %in% "Kentucky" ~ "KY",
      STATE %in% "Louisiana" ~ "LA",
      STATE %in% "Maine" ~ "ME",
      STATE %in% "Maryland" ~ "MD",
      STATE %in% "Massachusetts" ~ "MA",
      STATE %in% "Michigan" ~ "MI",
      STATE %in% "Minnesota" ~ "MN",
      STATE %in% "Mississipi" ~ "MS",
      STATE %in% "Missouri" ~ "MO",
      STATE %in% "Montana" ~ "MT",
      STATE %in% "Nebraska" ~ "NE",
      STATE %in% "Nevada" ~ "NV",
      STATE %in% "New Hampshire" ~ "NH",
      STATE %in% "New Jersey" ~ "NJ",
      STATE %in% "New Mexico" ~ "NM",
      STATE %in% "New York" ~ "NY",
      STATE %in% "North Carolina" ~ "NC",
      STATE %in% "North Dakota" ~ "ND",
      STATE %in% "Ohio" ~ "OH",
      STATE %in% "Oklahoma" ~ "OK",
      STATE %in% "Oregon" ~ "OR",
      STATE %in% "Pennsylvania" ~ "PA",
      STATE %in% "Puerto Rico" ~ "PR",
      STATE %in% "Rhode Island" ~ "RI",
      STATE %in% "South Carolina" ~ "SC",
      STATE %in% "South Dakota" ~ "SD",
      STATE %in% "Tennessee" ~ "TN",
      STATE %in% "Texas" ~ "TX",
      STATE %in% "Utah" ~ "UT",
      STATE %in% "Vermont" ~ "VT",
      STATE %in% "Virginia" ~ "VA",
      STATE %in% "Washington" ~ "WA",
      STATE %in% "West Virginia" ~ "WV",
      STATE %in% "Wisconsin" ~ "WI",
      STATE %in% "Wyoming" ~ "WY"
    )
  )

# Change colnames for easier matching
colnames(state_pop) <- c("State", colnames(state_pop[-1]))

# Select State and 2015 column
state_pop_2015 <- state_pop[, c("State", "POPESTIMATE2015")]

# City population modify state
city_pop <- city_pop |>
  mutate(
    State = case_when(
      State %in% "Alabama" ~ "AL",
      State %in% "Alaska" ~ "AK",
      State %in% "Arizona" ~ "AZ",
      State %in% "Arkansas" ~ "AR",
      State %in% "California" ~ "CA",
      State %in% "Colorado" ~ "CO",
      State %in% "Connecticut" ~ "CT",
      State %in% "Delaware" ~ "DE",
      State %in% "District of Columbia" ~ "DC",
      State %in% "Florida" ~ "FL",
      State %in% "Georgia" ~ "GA",
      State %in% "Hawaii" ~ "HI",
      State %in% "Idaho" ~ "ID",
      State %in% "Illinois" ~ "IL",
      State %in% "Indiana" ~ "IN",
      State %in% "Iowa" ~ "IA",
      State %in% "Kansas" ~ "KS",
      State %in% "Kentucky" ~ "KY",
      State %in% "Louisiana" ~ "LA",
      State %in% "Maine" ~ "ME",
      State %in% "Maryland" ~ "MD",
      State %in% "Massachusetts" ~ "MA",
      State %in% "Michigan" ~ "MI",
      State %in% "Minnesota" ~ "MN",
      State %in% "Mississipi" ~ "MS",
      State %in% "Missouri" ~ "MO",
      State %in% "Montana" ~ "MT",
      State %in% "Nebraska" ~ "NE",
      State %in% "Nevada" ~ "NV",
      State %in% "New Hampshire" ~ "NH",
      State %in% "New Jersey" ~ "NJ",
      State %in% "New Mexico" ~ "NM",
      State %in% "New York" ~ "NY",
      State %in% "North Carolina" ~ "NC",
      State %in% "North Dakota" ~ "ND",
      State %in% "Ohio" ~ "OH",
      State %in% "Oklahoma" ~ "OK",
      State %in% "Oregon" ~ "OR",
      State %in% "Pennsylvania" ~ "PA",
      State %in% "Puerto Rico" ~ "PR",
      State %in% "Rhode Island" ~ "RI",
      State %in% "South Carolina" ~ "SC",
      State %in% "South Dakota" ~ "SD",
      State %in% "Tennessee" ~ "TN",
      State %in% "Texas" ~ "TX",
      State %in% "Utah" ~ "UT",
      State %in% "Vermont" ~ "VT",
      State %in% "Virginia" ~ "VA",
      State %in% "Washington" ~ "WA",
      State %in% "West Virginia" ~ "WV",
      State %in% "Wisconsin" ~ "WI",
      State %in% "Wyoming" ~ "WY"
    )
  )

# Filter city population by 2015
city_pop_2015 <- city_pop[which(city_pop$year == 2015),]

# Concatenate supplementary datasets
supp_data <- left_join(hs_perc, median_income,
                       by = c("Geographic.Area" , "City"))

supp_data <- left_join(supp_data, poverty_perc,
                       by = c("Geographic.Area" , "City"))

supp_data <- left_join(supp_data, race_perc)
colnames(supp_data) <- c("State",
                         "City",
                         "HSDegree",
                         "Median",
                         "Poverty",
                         "White",
                         "Black",
                         "Native",
                         "Asian",
                         "Hispanic")

supp_data$City <- iconv(supp_data$City, to = "UTF-8")

# Remove " city" from the Location column
supp_data$City <- gsub(" city", "", supp_data$City)
supp_data$City <- gsub(" town", "", supp_data$City)
supp_data$City <- gsub(" CDP", "", supp_data$City)

# Merge supplementary data and state population data
supp_data <- left_join(supp_data, city_pop_2015,
                       by = c("State", "City"))
```


# Project Overview and Dataset

## Police Shootings: An overview

-   Police shootings are an unfortunate aspect of enforcing the law, as any loss of life is a tragic incident
-   We aim to visualize and analyze police shootings in the United States from 2015 to 2017
-   The data points were gathered on Kaggle and sourced from the Washington Post, so we are limited to their interpretation of what counts a as a police shooting

# Data Summary with Dashboard

<!--------------------------------------------------------------->

# Question 1

How do socioeconomic factors such as median income, poverty rates, and education relate with incidents of police use of force?

## Plot 1a

```{r}
#| label: Q1-plot1
#| warning: FALSE

# supp_data
supp_data <- na.omit(supp_data)

# Ensure the column names are correctly formatted before merging
fatality$city <- as.character(fatality$city)
fatality$state <- as.character(fatality$state)
supp_data$City <- as.character(supp_data$City)
supp_data$State <- as.character(supp_data$State)

# Merge the main fatality data with the supplementary socio-economic data
analysis_data <- merge(fatality,
                       supp_data,
                       by.x = c("city", "state"),
                       by.y = c("City", "State"))

# Convert Poverty from character to numeric to handle in quantile calculation
analysis_data$Poverty <- as.numeric(as.character(analysis_data$Poverty))

# Check for NA in Poverty which might disrupt quantile calculation
if (any(is.na(analysis_data$Poverty))) {
  analysis_data$Poverty[is.na(analysis_data$Poverty)] <- median(analysis_data$Poverty, na.rm = TRUE)
}

# Ensure Poverty is numeric
analysis_data$Poverty <- as.numeric(as.character(analysis_data$Poverty))

# Convert HSDegree to numeric, coercing non-numeric values to NA
analysis_data$HSDegree <- as.numeric(as.character(analysis_data$HSDegree))

# Calculate the range for Poverty and HSDegree
range_poverty <- max(analysis_data$Poverty, na.rm = TRUE) - min(analysis_data$Poverty, na.rm = TRUE)
range_hsdegree <- max(analysis_data$HSDegree, na.rm = TRUE) - min(analysis_data$HSDegree, na.rm = TRUE)

# Define consistent breaks based on the range
number_of_brackets <- 5 # For example, 5 equal brackets
breaks_poverty <- seq(min(analysis_data$Poverty, na.rm = TRUE), 
                      max(analysis_data$Poverty, na.rm = TRUE), 
                      length.out = number_of_brackets + 1)

breaks_hsdegree <- seq(min(analysis_data$HSDegree, na.rm = TRUE), 
                       max(analysis_data$HSDegree, na.rm = TRUE), 
                       length.out = number_of_brackets + 1)

# Function to create labels based on breaks
create_labels <- function(breaks) {
  labels <- c()
  for (i in 1:(length(breaks) - 1)) {
    labels <- c(labels, paste(formatC(breaks[i], format = "f", digits = 0), "-", formatC(breaks[i+1], format = "f", digits = 0), "%"))
  }
  return(labels)
}

# Use these breaks to make the cuts consistent
analysis_data$poverty_bracket <- cut(analysis_data$Poverty,
                                     breaks = breaks_poverty,
                                     include.lowest = TRUE,
                                     labels = create_labels(breaks_poverty))

analysis_data$edu_bracket <- cut(analysis_data$HSDegree,
                                 breaks = breaks_hsdegree,
                                 include.lowest = TRUE,
                                 labels = create_labels(breaks_hsdegree))

analysis_data$threat_level <- str_to_title(analysis_data$threat_level)

# Plotting for Poverty Bracket
poverty_plot  <- ggplot(analysis_data, aes(x = poverty_bracket, fill = threat_level)) +
  geom_bar(position = "identity", width = 0.5) +
  scale_fill_brewer(palette = 'Set3') + 
  theme_minimal() +
  labs(title = "Distribution of Threat Levels by Poverty Bracket",
       x = "Percentage of Population Below Poverty Line",
       y = "Count of Incidents",
       fill = "Threat Level") + 
  theme(legend.position = 'right')  # Adjusted theme settings here
poverty_plot
```

## Plot 1b

```{r}
#| label: Q1-plot2
#| warning: FALSE

# Plotting for Educational Bracket
education_plot <- ggplot(analysis_data, aes(x = edu_bracket, fill = threat_level)) +
  geom_bar(position = "identity", width = 0.5) +
  scale_fill_brewer(palette = 'Set3') + 
  theme_minimal() +
  labs(title = "Distribution of Threat Levels by High School Pass Rate",
       x = "Percentage of Population with High School Degree",
       y = "Count of Incidents",
       fill = "Threat Level") + 
  theme(legend.position = 'right')  # Adjusted theme settings here

education_plot

```

## Insights and Knowledge Inferred {style="font-size: 80%"}

-   Cities with over 75% high school graduates and poverty levels between 18-25% experience the highest number of shootings, predominantly with a threat level categorized as "Attack."

-   Most shootings across the board are recorded with a threat level of "Attack" or "Other," with only a few incidents left undetermined, indicating that officers generally report a justification for using their weapon.

-   The correlation between higher shootings and areas with more educated populations alongside a notable poverty rate might be influenced by large urban areas like New York City and Chicago, which have significant population densities and economic disparities.

-   Areas with lower percentages of high school graduates appear to be exceptions within the overall dataset, possibly acting as outliers.

<!--------------------------------------------------------------->

## Racial Shooting Density Across Median Income

```{r}
#| label: Q1-plot2a
#| message: false

# Calculate the populations of each population for each city
supp_stat <- na.omit(supp_data)
supp_stat <- supp_stat |>
  mutate(
    white_pop <- White * Population / 100,
    black_pop <- Black * Population / 100,
    native_pop <- Native * Population / 100,
    asian_pop <- Asian * Population / 100,
    hispanic_pop <- Hispanic * Population / 100
  )

# Select specific columns of the supplementary data
supp_stat <- supp_stat[,c(1, 2, 4, 15:19)]
colnames(supp_stat) <- c("State",
                         "City",
                         "Median",
                         "White",
                         "Black",
                         "Native",
                         "Asian",
                         "Hispanic")

# Select specific columns of the fatality data
fatality_subset <- fatality[,8:10]
colnames(fatality_subset) <- c("Race", "City", "State")

# Merge dataset and remove rows with NAs
merged_data <- left_join(fatality_subset, supp_stat,
                         by = c("State", "City"))
merged_data <- na.omit(merged_data)

# Acquire counts of race:
# Asian = 23, Black = 409, Hispanic = 301, Native American = 13, Other = 17,
# Unknown = 92, White = 466
race_count <- as.data.frame(summary(as.factor(merged_data$Race)))

# Refactor levels according to the total counts
merged_data$Race <- factor(merged_data$Race,
                           levels = c("Unknown",
                                      "Other",
                                      "Native American",
                                      "Asian",
                                      "Hispanic",
                                      "Black",
                                      "White"))

# Generate the density ridge plot (smallest count at the bottom)
ggplot(merged_data, aes(x = Median, y = Race)) +
  geom_density_ridges() +
  scale_x_continuous(expand = c(0, 0),
                    labels = label_dollar(scale = 1e-3, suffix = "K")) +
  coord_cartesian(clip = "off") +
  geom_text(aes(x = 125000, y = 1.2, label = "13")) +
  geom_text(aes(x = 125000, y = 2.2, label = "17")) +
  geom_text(aes(x = 125000, y = 3.2, label = "23")) +
  geom_text(aes(x = 125000, y = 4.2, label = "92")) +
  geom_text(aes(x = 125000, y = 5.2, label = "301")) +
  geom_text(aes(x = 125000, y = 6.2, label = "409")) +
  geom_text(aes(x = 125000, y = 7.2, label = "466")) +
  labs(title = "Density Plots of Police Shootings",
       subtitle = "Ordered by descending counts") +
  xlab("Median Income of City") +
  ylab("Race") +
  theme_minimal()
```

## Median Insight

```{r}
#| label: Q1-median
#| message: false

# Find distinct city and states and only keep numerical data
distinct_city <- merged_data |>
  distinct(State, City, .keep_all = TRUE) |>
  select(Median, White, Black, Native, Asian, Hispanic)

# Generate dataset for geom_smooth
distinct_city <- distinct_city |> # Find the proportions for each race
  mutate(
    Total = rowSums(across(White:Hispanic)),
    White = White / Total,
    Black = Black / Total,
    Native = Native / Total,
    Asian = Asian / Total,
    Hispanic = Hispanic / Total
  ) |> # Pivot data so race and values are columns
  pivot_longer(
    cols = c(White, Black, Native, Asian, Hispanic),
    names_to = "Race",
    values_to = "Percent"
  ) |> # Create legend column so that races can have colors later
  mutate(
    legend = case_when(
      Race %in% "White" ~ "White",
      Race %in% "Black" ~ "Black",
      TRUE ~ "Other"
    )
  )

# Refactor levels for legends order
distinct_city$legend <- factor(distinct_city$legend,
                               levels = c("Black", "White", "Other"))

# Generate smooth plot
ggplot(data = distinct_city,
       aes(x = Median, y = Percent, group = Race, color = legend)) +
  geom_smooth(se = F) +
  scale_color_manual(values = c("red", "blue", "grey")) +
  scale_x_continuous(labels = label_dollar(scale = 1e-3, suffix = "K")) +
  scale_y_continuous(labels = percent_format()) +
  coord_cartesian(xlim = c(35000, 75000)) +
  labs(title = "City Race Proportions",
       subtitle = "Over city median income",
       x = "Median Income of City",
       y = "Proportion of Population") +
  guides(color = guide_legend(title = "Race")) +
  theme_minimal() +
  theme()

```

## Adjusted Density Plots

```{r}
#| label: Q1-plot2b
#| message: false

# Find the race populations
# White = 135231935, Black = 30170898, Native = 1827873, Asian = 12570851,
# Hispanic = 51034800
race_pop <- merged_data |>
  distinct(State, City, .keep_all = TRUE)
race_total <- as.data.frame(colSums(race_pop[,5:9]))

# Proportion shot for each race:
# Asian:0.000182963
# race_count[1,1]/race_total[4,1] * 100
# Black: 0.001355611
# race_count[2,1]/race_total[2,1] * 100
# Hispanic: 0.0005897936
# race_count[3,1]/race_total[5,1] * 100
# Native: 0.0007112093
# race_count[4,1]/race_total[3,1] * 100
# White: 0.0003445932
# race_count[7,1]/race_total[1,1] * 100

# Refactor levels according to the proportions
merged_data$Race <- factor(merged_data$Race,
                           levels = c("Unknown",
                                      "Other",
                                      "Asian",
                                      "White",
                                      "Hispanic",
                                      "Native American",
                                      "Black"))

# Generate the density ridge plot (smallest proportion at the bottom)
ggplot(merged_data, aes(x = Median, y = Race)) +
  geom_density_ridges() +
  scale_x_continuous(expand = c(0, 0),
                    labels = label_dollar(scale = 1e-3, suffix = "K")) +
  coord_cartesian(clip = "off") +
  geom_text(aes(x = 125000, y = 3.2, label = "0.0002%")) +
  geom_text(aes(x = 125000, y = 4.2, label = "0.0003%")) +
  geom_text(aes(x = 125000, y = 5.2, label = "0.0006%")) +
  geom_text(aes(x = 125000, y = 6.2, label = "0.0007%")) +
  geom_text(aes(x = 125000, y = 7.2, label = "0.0014%")) +
  labs(title = "Density Plots of Police Shootings",
       subtitle = "Ordered by descending proportion within race") +
  xlab("Median Income of City") +
  ylab("Race") +
  theme_minimal()
```

## Insights and Knowledge Inferred {style="font-size: 80%"}

-   Most shootings occur in cities where the median income is roughly between \$40k - \$50k per year, roughly the US average

-   White people are shot the most, with the highest raw count, but proportional to the overall population of each race, black people are shot more

-   1 in every 71,429 black people are shot by the police compared to 1 in every 333,334 white people. Thus, black people are 4.7 times as likely to be shot by police, a striking and sad statistic <!--------------------------------------------------------------->

# Question 2

In what ways do the analyses of hotspot states, hotspot cities, and racial distribution contribute to understanding the intersection of incidents of police use of force across diverse geographic areas?

## Analysis Approach

::: {style="font-size: 60%;"}
**State and City Hotspots Analysis**:

-   Identifies regions and major urban areas with heightened police incidents using the shooting intensity metric, providing a nuanced understanding of localized dynamics.
-   Utilizes shooting intensity per million people to highlight states and cities with high incidents.
-   Incorporates custom annotation techniques to ensure clarity and comprehensive representation, including strategic labeling for Alaska on the maps.

**Racial Intensity Heatmap**:

-   Applies the same shooting intensity metric per million people to highlight racial disparities in police incidents across states.
-   Visualizes racial disparities in police incidents across states using color gradients.
-   Utilizes transitions to illustrate evolving racial dynamics over the years.
:::

## Plot 1

```{r}
#| label: Q2-plot1-state_shooting_intensity
#| message: false
#| eval: false
#| warning: FALSE
#| echo: FALSE

# Define data file paths
data_paths <- list(
  fatality = "data/PoliceKillingsUS.csv",
  state_pop = "data/Population_Estimate_data_Statewise_2010-2023.csv"
)

# Load data into a list
data_list <- lapply(data_paths, read.csv, na.strings = "")



# Define state abbreviations
state_abbreviations <- c(
  "Alabama" = "AL", "Alaska" = "AK", "Arizona" = "AZ", 
  "Arkansas" = "AR","California" = "CA", "Colorado" = "CO",
  "Connecticut" = "CT", "Delaware" = "DE","District of Columbia" = "DC",
  "Florida" = "FL", "Georgia" = "GA", "Hawaii" = "HI",
  "Idaho" = "ID", "Illinois" = "IL", "Indiana" = "IN", "Iowa" = "IA", 
  "Kansas" = "KS","Kentucky" = "KY", "Louisiana" = "LA", "Maine" = "ME", 
  "Maryland" = "MD", "Massachusetts" = "MA", "Michigan" = "MI", 
  "Minnesota" = "MN", "Mississippi" = "MS", "Missouri" = "MO", "Montana" = "MT",
  "Nebraska" = "NE", "Nevada" = "NV", "New Hampshire" = "NH", "New Jersey" = "NJ",
  "New Mexico" = "NM", "New York" = "NY", "North Carolina" = "NC", 
  "North Dakota" = "ND", "Ohio" = "OH", "Oklahoma" = "OK", "Oregon" = "OR", 
  "Pennsylvania" = "PA", "Rhode Island" = "RI", "South Carolina" = "SC", 
  "South Dakota" = "SD","Tennessee" = "TN", "Texas" = "TX", "Utah" = "UT", 
  "Vermont" = "VT", "Virginia" = "VA", "Washington" = "WA",
  "West Virginia" = "WV", "Wisconsin" = "WI", "Wyoming" = "WY", "Puerto Rico" = "PR"
)

# Convert state abbreviations to data frame
state_abbreviations_df <- data.frame(STATE = names(state_abbreviations), state = state_abbreviations, row.names = NULL)


# Process fatality data
fatality <- data_list[["fatality"]] %>%
  mutate(
    date = as.Date(date, format = "%d/%m/%y"),
    year = year(date)
  ) %>%
  select(-date) %>%
  group_by(state, year) %>%
  summarise(fatalities_count = n(), .groups = "drop")

# Process state population data
state_pop_updated <- data_list[["state_pop"]] %>%
  inner_join(state_abbreviations_df) %>%
  pivot_longer(
    cols = starts_with("POP"),
    names_to = "year",
    values_to = "population"
  ) %>%
  mutate(
    complete_year = as.Date(paste0("01/01/", gsub("POPESTIMATE", "", year)), format = "%d/%m/%Y")
  ) |>
  mutate(
    year = year(complete_year)
  )

# Merge fatality summaries with population data
fatalities_population <- fatality %>%
  inner_join(state_pop_updated, by = c("state", "year")) %>%
  mutate(
    intensity = (fatalities_count / population) * 1000000,
    region = tolower(STATE)
  )

# Function to prepare label data
prepare_label_data <- function(map_data, fatalities_data) {
  # Define DC row
  dc_row <- data.frame(long = -77.0369,
                       lat = 38.9072,
                       state = "District of Columbia",
                       region = tolower("District of Columbia"))
  
  # Define label data
  label_data <- data.frame(long = state.center$x,
                           lat = state.center$y,
                           state = state.name) %>%
    mutate(region = tolower(state)) %>%
    bind_rows(dc_row)
  
  # Join with fatalities data
  label_data <- label_data %>%
    left_join(fatalities_data, by = c("region" = "region")) %>%
    mutate_at(vars(matches("\\d{4}$")), list(~paste0(., "_", fatalities_data$year)))
  
  # Ensuring no duplicate names
  names(label_data)[3] <- "state"
  names(label_data)[5] <- "US_STATE"
  names(label_data)[8] <- "USSTATE"

  return(label_data)
}


# Function to prepare map data for Alaska
prepare_alaska_map_data <- function(us_map_data) {
  alaska <- map_data("world") %>%
    filter(region == "USA" & subregion == "Alaska") %>%
    mutate(region = tolower(subregion),
           order = max(us_map_data$order) + 1,
           long = ifelse(long < 0, long, -150),
           lat = ifelse(long < 0, lat, 65))
  
  return(alaska)
}


# Creating a state wise plotting of shooting intensity since we need to add alaska map seperately each year.
plot_shooting_intensity <- function(fatalities_population, us_map_data, year, label_data) {
  
  # Filter fatalities data for the specific year
  fatalities_population <- fatalities_population[fatalities_population$year == year, ]
  
  # Filter label data for the specific year
  label_data <- label_data[label_data$year == year, ]
  
  # Scalingup the data by 1.75 time for year 2017 as data for full year is not available
  if ( year == 2017) {
    fatalities_population$intensity <- fatalities_population$intensity * 1.75
    label_data$intensity <- label_data$intensity * 1.75
  }
  
  # Creating Alaska Labels
  alaska_label <- label_data |>
    filter(
      state %in% c("Alaska")
    ) |>
    mutate(
      long = case_when(
        state == "Alaska" ~ -150,
        state != "Alaska" ~ long
      ),
      lat = case_when(
        state == "Alaska" ~ 65,
        state != "Alaska" ~ lat
      )
    )
  
  label_data <-label_data |>
    filter(
      !(state %in% c("Hawaii", "Alaska"))
    )

    
  # Merge with map data that can be used for plotting
  merge_data <- full_join(us_map_data, fatalities_population %>% 
                            filter(!(region %in% c("alaska", "hawaii", "rhode island"))))
  
  # Filter Alaska data to use in alaska map plotting
  alaska_data <- fatalities_population %>% 
    filter(region %in% c("Alaska"))
  

  # Ploting the usa map using merged data
  p <- ggplot() +
    geom_polygon(data = merge_data,
                 aes(x = long, y = lat, group = group, fill = intensity),
                 color = "white", size = 0.2) +
    scale_fill_gradient(low = "#fee5d9", high = "#a50f15", na.value = "grey50",
                        name = "Intensity") +
    geom_label_repel(data = label_data %>% filter(intensity > 3), 
                     aes(x = long, y = lat, 
                         label = paste(state,":\n",round(intensity,1))), size = 5) +
    theme_void() 
  
  # Add Alaska data if available
  if (!is.null(alaska_data)) {
    
    # Creating another plot using alaska data
    p1 <- ggplot() + geom_polygon(data = prepare_alaska_map_data(us_map_data),
                          aes(x = long, y = lat, group = group),
                          fill = "#e69e91", color = "white", linewidth = 0.2) +
      geom_label_repel(data = alaska_label,
                       aes(x = long, y = lat, label = paste(state, ":\n", round(intensity, 1))),
                       size = 5) +
      coord_cartesian(xlim = c(-180, 0), ylim = c(0, 90)) +
      theme_void() +
      theme(legend.position = "none") 
    
    # Add alaska plot at the bottom to usa map
    p <- p + annotation_custom(ggplotGrob(p1), xmin = -130, xmax = -70, ymin = 0, ymax = 40) +
      labs(title = paste("Shooting Intensity by State :", year),
           subtitle = "Per Million of Population",
           caption = paste("Highlighted states have shooting intensity > 3 : #", length(fatalities_population$intensity[fatalities_population$intensity>3]),
                           "\n Grey states indicate no fatalties in that year")
      ) +
      theme(
        legend.position = "bottom",
        plot.title = element_text(hjust = 0.5, size = 20, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5, size = 16),
        plot.caption = element_text(size = 10)
      ) +
      annotate(
        "text",
        x = -70,
        y = 30,
        label = "Top 3 by Total Fatalities:
    1) California
    2) Texas
    3) Florida",
        size = 5,
        fontface = "italic",
        color = "#a50f15",
        vjust = 0
      )
  
  }
  
  return(p)
}

# loading usa map
us_map_data <- map_data("state")

# Prepare label data
label_data <- prepare_label_data(us_map_data, fatalities_population)

# Plot state shooting intensity for 2015  and adding specific  annotation
plot_2015_map <- 
  plot_shooting_intensity(fatalities_population, us_map_data, 2015, label_data) + 
  annotate(
    "text",
    x = -80,
    y = 45,
    label = "Top 3 by Shooting Intensity:
    1) Wyoming
    2) New Mexico
    3) Oklahoma",
    size = 5,
    fontface = "italic",
    color = "#a50f15",
    vjust = 0
  ) 

# Plot state shooting intensity for 2016  and adding specific  annotation
plot_2016_map <- 
  plot_shooting_intensity(fatalities_population, us_map_data, 2016, label_data) + 
  annotate(
    "text",
    x = -80,
    y = 45,
    label = "Top 3 by Shooting Intensity:
    1) New Mexico
    2) Alaska
    3) District of Columbia",
    size = 5,
    fontface = "italic",
    color = "#a50f15",
    vjust = 0
  ) 

# Plot state shooting intensity for 2017  and adding specific  annotation
plot_2017_map <- 
  plot_shooting_intensity(fatalities_population, us_map_data, 2017, label_data) +
  annotate(
    "text",
    x = -80,
    y = 45,
    label = "Top 3 by Shooting Intensity:
    1) Maine
    2) Alaska
    3) Oklahoma",
    size = 5,
    fontface = "italic",
    color = "#a50f15",
    vjust = 0
  ) 


# Animating the three plots created above
animation::saveGIF(
  expr = {
    plot(plot_2015_map)
    plot(plot_2016_map)
    plot(plot_2017_map)
  },
  movie.name = "images/shooting_intensity.gif",
  interval = 4,
  ani.width = 1200,
  ani.height = 700,
  ani.dev = "png",
  ani.type = "cairo",
  res = 300,
  ani.x = "in",
  ani.y = "in",
  ani.unit = "in",
  ani.bg = "white",
  ani.fps = 3
)

```

![](images/shooting_intensity.gif){fig-align="center"}

## Plot 2

```{r}
#| label: Q2-plot2-city_shooting_intensity
#| message: false
#| eval: false
#| warning: FALSE
#| echo: FALSE

# Process city fatality data
city_fatality_summaries_year <- data_list[["fatality"]] %>%
  mutate(
    year = year(as.Date(date, format = "%d/%m/%y"))
  ) %>%
  group_by(state, city, year) %>%
  summarise(fatalities_count = n(), .groups = "drop")

# Filter city fatality data for 2015 and 2016
city_fatality_summaries_year <- city_fatality_summaries_year %>%
  filter(year %in% c(2015, 2016)) %>%
  group_by(city, year) %>%
  slice_max(order_by = fatalities_count) %>%
  ungroup()

# Merge city fatality data with city population data
city_fatalities_population <- inner_join(city_fatality_summaries_year, data_list[["city_pop"]], by = c("city" = "City", "year" = "year")) %>%
  mutate(
    intensity = (fatalities_count / Population) * 1000000,
    region = tolower(State)
  )

# Creating a city wise plotting of shooting intensity since we need to add Alaska map separately each year
# This code uses the earlier state wise heat intensity
plot_shooting_intensity_city <- function(fatalities_population, us_map_data, year, city_population_data) {
  
  # Filter fatalities data for the specific year
  fatalities_population <- fatalities_population[fatalities_population$year == year, ]
  
  # Filter City data for the specific year
  city_population_data <- city_population_data[city_population_data$year == year, ]
  
  # Merge with map data
  merge_data <- full_join(us_map_data, fatalities_population %>% 
                            filter(!(region %in% c("alaska", "hawaii", "rhode island"))))
  
  # Filter Alaska data
  alaska_data <- fatalities_population %>% 
    filter(region %in% c("Alaska"))
  
  
  # Plot map
  p <- ggplot() +
    geom_polygon(data = merge_data,
                 aes(x = long, y = lat, group = group, fill = intensity),
                 color = "white", size = 0.2) +
    scale_fill_gradient(low = "#fee5d9", high = "#a50f15", na.value = "grey50",
                        name = "Intensity") +
    theme_void() 
  
  # Add Alaska data if available
  if (!is.null(alaska_data)) {
    
    p1 <- ggplot() + geom_polygon(data = prepare_alaska_map_data(us_map_data),
                                  aes(x = long, y = lat, group = group),
                                  fill = "#e69e91", color = "white", linewidth = 0.2) +
      coord_cartesian(xlim = c(-180, 0), ylim = c(0, 90)) +
      theme_void() +
      theme(legend.position = "none") +
      geom_point(data = subset(city_population_data,city_population_data$region=="alaska"),
                 aes(x = lon,y=lat), color = "darkred")
    
    # Add custom annotation if provided
    p <- p + annotation_custom(ggplotGrob(p1), xmin = -130, xmax = -70, ymin = 0, ymax = 40) +
      geom_point(data = subset(city_population_data,city_population_data$lon>-140),
                 aes(x = lon,y=lat, size = intensity),
                 color = "#8B0020") +
      geom_label_repel(data = city_population_data %>% filter(intensity > 28), 
                       aes(x = lon, y = lat, label = paste(city,"\n",round(intensity,1))), 
                       #color = "#56007C",
                       color = "#000000",
                       nudge_x = ifelse(city_population_data$city == "Oakland", -2, -0.5),
                       nudge_y = ifelse(city_population_data$city == "Oakland", -1, -0.5),
                       size = 5, 
                       fill = "transparent") +
      labs(title = paste("Shooting Intensity by City :", year),
           subtitle = "Per Million of Population",
           caption = "Cities labels are of shooting intensity > 28
           Grey highlighted states have NO fatalities"
      ) +
      theme(
        legend.text = element_text(size = 9, color = "black"), 
        legend.title = element_text(size = 12),
        plot.title = element_text(hjust = 0.5, size = 24, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5, size = 18),
        plot.caption = element_text(size = 10)
      )
  }
  return(p)
}

# loading usa map
us_map_data <- map_data("state")

# Plot shooting intensity for 2015 for city
map_2015_city <- plot_shooting_intensity_city(fatalities_population, us_map_data, 2015, city_fatalities_population)  

# Plot shooting intensity for 2016 for city
map_2016_city <- plot_shooting_intensity_city(fatalities_population, us_map_data, 2016, city_fatalities_population) 

# Animating the two plots created above
animation::saveGIF(
  expr = {
    plot(map_2015_city)
    plot(map_2016_city)
  },
  movie.name = "images/shooting_intensity_city.gif",
  interval = 4,
  ani.width = 1200,
  ani.height = 700,
  ani.dev = "png",
  ani.type = "cairo",
  res = 300,
  ani.x = "in",
  ani.y = "in",
  ani.unit = "in",
  ani.bg = "white",
  ani.fps = 3
)

```

![](images/shooting_intensity_city.gif){fig-align="center"}

## Plot 3

```{r}
#| label: Q.2_Plot_3_racial_intensity
#| message: false
#| eval: false
#| warning: FALSE
#| echo: FALSE

state_pop <- read.csv("data/Population_Estimate_data_Statewise_2010-2023.csv")
fatality$year <- format(as.Date(fatality$date, format = "%d/%m/%y"), "%Y")

# Group by year, state, and race, then count occurrences
race_count <- fatality |>
  group_by(year, state, race) |>
  summarise(count = n()) |>
  ungroup()

# Spread the 'race' column to create individual columns for each race
race_count <- race_count |>
  spread(key = race, value = count, fill = 0)

# Add a column for the total count of races in each state and year
race_count <- race_count |>
  mutate(Total = rowSums(select(., -c(year, state)), na.rm = TRUE))

# Arrange the data by year and state
race_count <- race_count |>
  arrange(year, state)
race_perc <- read.csv("data/ShareRaceByCity.csv")

# Remove the 'City' column from race_perc dataframe
race_perc <- race_perc |>
  select(-City)

# Convert columns to numeric
race_perc$share_white <- as.numeric(race_perc$share_white)
race_perc$share_black <- as.numeric(race_perc$share_black)
race_perc$share_native_american <- as.numeric(race_perc$share_native_american)
race_perc$share_asian <- as.numeric(race_perc$share_asian)
race_perc$share_hispanic <- as.numeric(race_perc$share_hispanic)

# Add a column for the sum of all race percentages
race_perc$sum_race_percentages <- rowSums(race_perc[, c("share_white", "share_black", "share_native_american", "share_asian", "share_hispanic")])

# Assuming race_perc is already loaded as a dataframe
race_perc <- race_perc |>
  group_by(Geographic.area) |>
  summarise(
    share_white = mean(share_white, na.rm = TRUE),
    share_black = mean(share_black, na.rm = TRUE),
    share_native_american = mean(share_native_american, na.rm = TRUE),
    share_asian = mean(share_asian, na.rm = TRUE),
    share_hispanic = mean(share_hispanic, na.rm = TRUE),
    sum_race_percentages=mean(sum_race_percentages,na.rm = TRUE)
  )

# Normalize race percentages
race_perc <- race_perc |>
  mutate(
    share_white = share_white / sum_race_percentages,
    share_black = share_black / sum_race_percentages,
    share_native_american = share_native_american / sum_race_percentages,
    share_asian = share_asian / sum_race_percentages,
    share_hispanic = share_hispanic / sum_race_percentages
  )

# Recalculate sum_race_percentages
race_perc$sum_race_percentages <- rowSums(select(race_perc, starts_with("share_")))

# Selecting desired columns from state_pop
state_pop_subset <- state_pop[, c("STATE",
                                  "POPESTIMATE2015",
                                  "POPESTIMATE2016",
                                  "POPESTIMATE2017")]

# Reshape the dataframe to have Year, State, and Population columns
state_pop_long <- state_pop_subset |>
  pivot_longer(cols = c("POPESTIMATE2015", "POPESTIMATE2016", "POPESTIMATE2017"),
               names_to = "Year",
               values_to = "Population")

# Remove "POPESTIMATE" from the Year column
state_pop_long <- state_pop_long |>
  mutate(Year = str_remove(Year, "POPESTIMATE"))

# Convert Year column to numeric for sorting
state_pop_long$Year <- as.numeric(state_pop_long$Year)

# Sort the dataframe by Year in ascending order
state_pop_long <- state_pop_long |>
  arrange(Year)

# Define a mapping of state names to state codes
state_mapping <- c(
  "Alabama" = "AL", "Alaska" = "AK", "Arizona" = "AZ", "Arkansas" = "AR",
  "California" = "CA", "Colorado" = "CO", "Connecticut" = "CT", "Delaware" = "DE",
  "District of Columbia" = "DC", "Florida" = "FL", "Georgia" = "GA", "Hawaii" = "HI",
  "Idaho" = "ID", "Illinois" = "IL", "Indiana" = "IN", "Iowa" = "IA",
  "Kansas" = "KS", "Kentucky" = "KY", "Louisiana" = "LA", "Maine" = "ME",
  "Maryland" = "MD", "Massachusetts" = "MA", "Michigan" = "MI", "Minnesota" = "MN",
  "Mississippi" = "MS", "Missouri" = "MO", "Montana" = "MT", "Nebraska" = "NE",
  "Nevada" = "NV", "New Hampshire" = "NH", "New Jersey" = "NJ", "New Mexico" = "NM",
  "New York" = "NY", "North Carolina" = "NC", "North Dakota" = "ND", "Ohio" = "OH",
  "Oklahoma" = "OK", "Oregon" = "OR", "Pennsylvania" = "PA", "Rhode Island" = "RI",
  "South Carolina" = "SC", "South Dakota" = "SD", "Tennessee" = "TN", "Texas" = "TX",
  "Utah" = "UT", "Vermont" = "VT", "Virginia" = "VA", "Washington" = "WA",
  "West Virginia" = "WV", "Wisconsin" = "WI", "Wyoming" = "WY", "Puerto Rico" = "PR"
)

# Add STATE CODE column to state_pop dataframe
state_pop_long <- mutate(state_pop_long, STATE_CODE = state_mapping[STATE])

# Rearrange columns with STATE CODE after STATE
state_pop_long <- state_pop_long |>
  select(STATE, STATE_CODE, everything())

# Merge race_perc and state_pop_long dataframes based on state information
merged_data <- merge(race_perc, state_pop_long, by.x = "Geographic.area", by.y = "STATE_CODE")

# Convert race percentage columns to numeric
percentage_cols <- c("share_white", "share_black", "share_native_american", "share_asian", "share_hispanic")
merged_data[percentage_cols] <- lapply(merged_data[percentage_cols], as.numeric)
merged_data <- merged_data |>
  mutate(
    white_population = share_white * Population,
    black_population = share_black * Population,
    native_american_population = share_native_american * Population,
    asian_population = share_asian * Population,
    hispanic_population = share_hispanic * Population
  )

# Select the desired columns
merged_data <- merged_data |>
  select(Geographic.area, Year, Population, white_population, black_population, 
         native_american_population, asian_population, hispanic_population)

fatality <- read.csv("data/PoliceKillingsUS.csv", na.strings = "")
fatality$year <- format(as.Date(fatality$date, format = "%d/%m/%y"), "%Y")

# Keep only the required columns
fatality <- fatality |>
  select(race,state,year)

# Pivot the data wider
fatality_spread <- fatality |>
  pivot_wider(
    id_cols = c(state, year),
    names_from = race,
    values_from = race,
    values_fn = length,
    values_fill = 0
  )

# Rename the racial columns
fatality_spread <- fatality_spread |>
  rename(
    share_white = W,
    share_black = B,
    share_native_american = 'NA',
    share_asian = A,
    share_hispanic = H
   )

# Merge "N" and "O" columns into "Other" column
fatality_spread <- fatality_spread |>
  mutate(Other = N + O) |>
  select(-N, -O)

# Rename 'Geographic.area' to 'state' in grouped_merged_data for consistency
merged_data <- merged_data |>
  rename(state = Geographic.area)

# Convert Year column in grouped_merged_data to character
merged_data <- merged_data |>
  mutate(Year = as.character(Year))

# Perform left join
Final_racial_data <- left_join(merged_data, fatality_spread, by = c("state" = "state", "Year" = "year"))

# Calculate racial intensity for each racial group
Final_racial_data <- Final_racial_data |>
  mutate(
    white_intensity = round((share_white / white_population) * 10^6, 2),
    black_intensity = round((share_black / black_population) * 10^6, 2),
    native_american_intensity = round((share_native_american / native_american_population) * 10^6, 2),
    asian_intensity = round((share_asian / asian_population) * 10^6, 2),
    hispanic_intensity = round((share_hispanic / hispanic_population) * 10^6, 2)
  )

# Create a template dataframe with all combinations of states and years
template <- expand.grid(state = unique(Final_racial_data$state), 
                        Year = c("2015", "2016", "2017"))


# Merge the template dataframe with Final_racial_data
Final_racial_data <- merge(template, Final_racial_data, by = c("state", "Year"), all.x = TRUE)

# Replace NA values with 0
Final_racial_data[is.na(Final_racial_data)] <- 0

# Reorder columns to match the original dataframe
Final_racial_data <- Final_racial_data[, names(Final_racial_data)]

# Define the state_mapping_reversed dictionary
state_mapping_reversed <- c(
  "AL" = "Alabama", "AK" = "Alaska", "AZ" = "Arizona", "AR" = "Arkansas",
  "CA" = "California", "CO" = "Colorado", "CT" = "Connecticut", "DE" = "Delaware",
  "DC" = "District of Columbia", "FL" = "Florida", "GA" = "Georgia", "HI" = "Hawaii",
  "ID" = "Idaho", "IL" = "Illinois", "IN" = "Indiana", "IA" = "Iowa",
  "KS" = "Kansas", "KY" = "Kentucky", "LA" = "Louisiana", "ME" = "Maine",
  "MD" = "Maryland", "MA" = "Massachusetts", "MI" = "Michigan", "MN" = "Minnesota",
  "MS" = "Mississippi", "MO" = "Missouri", "MT" = "Montana", "NE" = "Nebraska",
  "NV" = "Nevada", "NH" = "New Hampshire", "NJ" = "New Jersey", "NM" = "New Mexico",
  "NY" = "New York", "NC" = "North Carolina", "ND" = "North Dakota", "OH" = "Ohio",
  "OK" = "Oklahoma", "OR" = "Oregon", "PA" = "Pennsylvania", "RI" = "Rhode Island",
  "SC" = "South Carolina", "SD" = "South Dakota", "TN" = "Tennessee", "TX" = "Texas",
  "UT" = "Utah", "VT" = "Vermont", "VA" = "Virginia", "WA" = "Washington",
  "WV" = "West Virginia", "WI" = "Wisconsin", "WY" = "Wyoming", "PR" = "Puerto Rico"
)

# Replace values in the state column using state_mapping_reversed
Final_racial_data$state <- state_mapping_reversed[as.character(Final_racial_data$state)]

# Correcting the use of pivot_longer
long_data <- Final_racial_data |>
  
    select(state, Year, ends_with("intensity")) |>  # Select columns that end with 'intensity' and include state, Year
  pivot_longer(
    cols = ends_with("intensity",FALSE),  # This selects all columns that end with 'intensity'
    names_to = "racial_group",
    values_to = "intensity"
  ) |>
  drop_na(intensity)  # Drop NA values in the intensity column

# Remove "intensity" from racial_group column
long_data <- long_data |>
  mutate(racial_group = gsub("_intensity", "", racial_group))

# Convert Year to character
long_data <- long_data |>
  mutate(Year = as.character(Year))

# Animation for Heatmap_racial_intesity 
animated_heatmap <- ggplot(long_data,
                           aes(x = str_to_title(racial_group),
                               y = state,
                               fill = intensity)) +
  geom_tile() +
  scale_fill_viridis_c(option = "plasma") +
  labs(title = "State vs. Racial Intensity Heatmap: Year {frame_time}", 
       x = "Racial Group", 
       y = "State",
       fill = "Intensity") +  # Set the legend title to "Intensity"
  theme_minimal() +
  theme(axis.text.x = element_text(size=16, angle = 0, hjust = 0.5), 
        axis.text.y = element_text(size = 14),
        axis.title = element_text(size = 20),
        plot.title = element_text(size = 20),
        panel.grid = element_blank(),
        legend.title = element_text(size = 16, face = "bold"),  # Increase legend title size and make it bold
        legend.text = element_text(size = 16),  # Increase legend text size
        legend.key.width = unit(1.6, "cm"),  # Increase legend key width
        legend.key.height = unit(1.6, "cm")) +  # Increase legend key height
  transition_time(as.integer(Year)) +  # Ensure 'Year' is treated as an integer
  ease_aes('linear')

# Render and save the animation
animate(animated_heatmap, fps = 1, duration = 15, width = 800, height = 800, renderer = gifski_renderer())
anim_save("images/racial_intensity_heatmap.gif")

```

![](images/racial_intensity_heatmap.gif){fig-align="center" width="1080"}

## Insights and Knowledge Inferred

::: {style="font-size: 60%;"}
-   *Plot 1*: Maps shooting intensity across all US states from 2015 to 2017, revealing varying levels of intensity with darker shades indicating higher rates and lighter shades lower rates, while gray marks states with no fatalities. Top states for shooting intensity and total fatalities are highlighted annually.
-   *Plot 2*: Depicts police shooting intensity and fatalities in major US cities for 2015-2017 per million population. Cities with higher intensity are represented by larger dots, with labels identifying those exceeding an intensity of 28, notably including Chicago and Los Angeles in multiple years.
-   *Plot 3*: Examines state-wise shootings by racial demographics, uncovering disparities in intensity. While white and Hispanic populations experience lower intensity, black and Native American communities consistently demonstrate higher levels, with an exception in Asian demographics in South Dakota in 2017.
-   *Discussion Summary*: Analyses of hotspot states, cities, and racial distribution offer insights into the intersection of police use of force incidents across diverse geographic areas, revealing dynamic shifts, urban significance, and racial disparities. These insights contribute to a nuanced understanding of localized dynamics and broader discussions on social equity and justice.
:::

# THANK YOU {background-image="images/thankyoubackground.png" background-size="1200px"}
